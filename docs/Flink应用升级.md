[Parent](../README.md)

# Flink应用升级

## 前提

* Flink默认情况下为每个算子生成一个uid，根据算子的出现顺序
* uid的作用是在应用重启时，匹配算子状态，进行恢复
* 自动生成的uid极其不稳定，修改代码后，从savepiont恢复应用时，很容易失败

所有算子，必须赋予UID，禁用自动uid代码设置

```scala
  env.getConfig.disableAutoGeneratedUIDs()
```

## 触发保存点

* 在Flink Dashboard找到jobid
* 在yarn上，找到applicationId
* 在hdfs，新建一个有权限写的路径

以yarn模式为例：

```shell script
# 停下应用，并触发保持点，将日志保存，方便后续查找目录
nohup ./bin/flink cancel \
-s hdfs://nameservice1/user/username/flink/savepoint/jobname \
-yid applicaionId jobid > cancel.log 2>&1 &

#手动触发保持点，不停应用
./bin/flink savepoint flinkJobId \
hdfs://nameservice1/user/username/flink/savepoint/jobname \
-yid applicaionId
```
## 启动应用

* -s 指定上一步的路径
* --allowNonRestoredState 允许异常启动
* -Dyarn.provided.lib.dirs指定hdfs flink lib目录

```shell script
# 维护，升级，启动脚本，yarn application模式
nohup ./bin/flink run-application -t yarn-application \
-Djobmanager.memory.process.size=2048m \
-Dtaskmanager.memory.process.size=2048m \
-Dyarn.provided.lib.dirs="hdfs flink lib dir" \
-c 'full jobClassName' \
-s savepointdir \
--allowNonRestoredState \
hdfs://nameservice1/user/username/flink/jobs/user.jar >app.log 2>&1 
```

## 内部算子状态



## 参考地址

* https://ci.apache.org/projects/flink/flink-docs-release-1.11/ops/upgrading.html
* https://ci.apache.org/projects/flink/flink-docs-stable/ops/state/savepoints.html